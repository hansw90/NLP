{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "자연어를 컴퓨터가 이해하고, 효율적으로 처리하게 하기 위해서는 컴퓨터가 이해할 수 있도록 자연어를 적절히 변환할 필요가 있습니다. 단어를 표현하는 방법에 따라서 자연어 처리의 성능이 크게 달라지기 때문에 이에 대한 많은 연구가 있었고, 여러가지 방법들이 알려져 있습니다.\n",
    "\n",
    "최근에는 단어의 의미를 벡터화시킬 수 있는 이번 챕터에서 배우게 될 워드투벡터(Word2Vec)와 글로브(Glove)가 많이 사용되고 있습니다. 이번 챕터에서는 전통적 방법의 한계를 개선시킨 워드 임베딩(Word Embedding) 방법론에 대해서 배워보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어 Word2vec 만들기\n",
    "\n",
    "영어 데이터를 다운받아 직접 Word2Vec 작업을 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression\n",
    "from lxml import etree\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273424\n",
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
     ]
    }
   ],
   "source": [
    "targetXML=open('datasets/ted_en-20160408/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (audio) (laughter) 등의 배경음 부분 제거\n",
    "# 해당코드는 괄호로 구성된 나용을 제거 한다\n",
    "\n",
    "sent_text = sent_tokenize(content_text)\n",
    "# 입력 코퍼스에 대해서 NLTK 를 이용하여 문장 토큰화를 수행\n",
    "\n",
    "\n",
    "print(len(sent_text))\n",
    "normalized_text = []\n",
    "\n",
    "for string in sent_text :\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "    \n",
    "result = []\n",
    "\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화 수행\n",
    "\n",
    "\n",
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273424\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "\n",
    "# size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "# window = 컨텍스트 윈도우 크기\n",
    "# min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "# workers = 학습을 위한 프로세스 수\n",
    "# sg = 0은 CBOW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=21613, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# model 의 형태를 보자\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8423302173614502), ('guy', 0.8012086749076843), ('lady', 0.789531946182251), ('girl', 0.7496041655540466), ('boy', 0.7466311454772949), ('gentleman', 0.7353699207305908), ('soldier', 0.7269606590270996), ('rabbi', 0.6808528900146484), ('kid', 0.6792615652084351), ('poet', 0.6763109564781189)]\n"
     ]
    }
   ],
   "source": [
    "a = model.wv.most_similar(\"man\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 글로브(GloVe)\n",
    "\n",
    "글로브 (Global Vectors for Word Representaion) 는 카운트 기반과 예측 기반을 모두 사용하는 방법의 단어 임베딩 방법론이다.\n",
    "\n",
    "카운트 기반의 LSA 와 예측기반의 Word2vec 의 단점을 보완 하기 위해 나왔다.\n",
    "\n",
    "lsa 는 dtm 이나 tf-idf 행렬과 같이 각 문서에서 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력어로 받아 차원을 축소 (Truncated SVC) 하여 잠재된 의미를 끌어내는 방법이였다.\n",
    "\n",
    "Word2vec 은 실제 값과 예측값에 대한 오차를 손실 함수를 통해 줄여가며 학습하는 예측기반 방법론이다.\n",
    "\n",
    "LSA 는 전체적인 통계 정보를 고려하지만 의미 유추 작업에는 성능이 떨어진다.\n",
    "\n",
    "Word2Vec 는 예측 기반으로 단어 유추작업에는 LSA 보다 뛰어나지만 임베딩 벡터가 윈도우 크기내에서만 주변 단어를 고려하기 떄문에 코퍼스 전체적인 통계 정보를 고려하지 못한다/\n",
    "\n",
    "GloVe 는 카운트 기반과 예측 기반 방법론 두가지 모두를 사용한다.\n",
    "\n",
    "\n",
    "GloVe는 '임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것이다' 정확히는 이를 만족하는 임베딩 벡터를 만드는 과정이다.\n",
    "\n",
    "    \n",
    "    dot(w_i,w_k) ≈ P(k|i)\n",
    "    \n",
    "    w_i : 중심 단어 i의 임베딩 벡터\n",
    "    w_k : 주변 단어 k의 임베딩 벡터\n",
    "    Pik  : P(k | i) = XikXi : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률\n",
    "    \n",
    "    dot product(wi wk~)≈ log P(k | i)=log Pik\n",
    "    \n",
    "\n",
    "최종 유도 :\n",
    "- Loss function = 1∑V   f(X_mn)(wT_m * wn + b_m + b_n − log(X_mn))^2\n",
    "\n",
    "f(X_mn) 는 뭐야??? : 동시 등장 행렬에서 동시 등장 빈도의 값 Xik이 굉장히 낮은 경우에는 정보에 거의 도움이 되지 않는다고 판단합니다. 그래서 이에 대한 가중치를 주는 고민을 하게 되는데 GloVe 연구팀이 선택한 것은 바로 Xik의 값에 영향을 받는 가중치 함수(Weighting function) f(Xik)를 손실 함수에 도입하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained word embeddings in a Keras model\n",
    "\n",
    "훈련 데이터가 적다면 케라스의 embedding() 을 사용하는 것보다 다른 텍스트 데이터로 사전 훈련되어 있는 임베딩 벡터를 불러오는 것이 나은 선택이다.\n",
    "\n",
    "keras 블로그 주소 : https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html \n",
    "위 페이지 예제를 실습 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe word embeddings\n",
    "\n",
    "\n",
    "We will be using GloVe embeddings, which you can read about here. GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics.\n",
    "\n",
    "Specifically, we will use the 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. You can download them here (warning: following this link will start a 822MB download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMO\n",
    "\n",
    "\n",
    "ELMo는 Embeddings from Language Model의 약자입니다. 해석하면 '언어 모델로 하는 임베딩'입니다. ELMo의 가장 큰 특징은 사전 훈련된 언어 모델(Pre-trained language model)을 사용한다는 점입니다. 이는 ELMo의 이름에 LM이 들어간 이유입니다.\n",
    "\n",
    "Bank라는 단어를 생각해봅시다. Bank Account(은행 계좌)와 River Bank(강둑)에서의 Bank는 전혀 다른 의미를 가지는데, Word2Vec이나 Glove 등으로 표현된 임베딩 벡터들은 이를 제대로 반영하지 못한다는 단점이 있습니다. 예를 들어서 Word2Vec이나 Glove 등의 임베딩 방법론으로 Bank란 단어를 [0.2 0.8 -1.2]라는 임베딩 벡터로 임베딩하였다고 하면, 이 단어는 Bank Account(은행 계좌)와 River Bank(강둑)에서의 Bank는 전혀 다른 의미임에도 불구하고 두 가지 상황 모두에서 [0.2 0.8 -1.2]의 벡터가 사용됩니다.\n",
    "\n",
    "그렇다면 같은 표기의 단어라도 문맥에 따라서 다르게 워드 임베딩을 할 수 있으면 자연어 처리의 성능이 더 올라가지 않을까요? 단어를 임베딩하기 전에 전체 문장을 고려해서 임베딩을 하겠다는 것이죠. 그래서 탄생한 것이 문맥을 반영한 워드 임베딩(Contextualized Word Embedding)입니다.\n",
    "\n",
    "## ELMo표현을 사용해서 스팸 메일 분류하기\n",
    "\n",
    "텐서플로우 허브로부터 다양하 사전 훈련된 모델(Pre-trained Model)들을 사용할 수 있다. 여기서는 사전훈련된 모델로부터 ELMo 표현을 사용해보는 정도로 예제를 진행해 보겠다, 시작전에 텐서플로우 허브를 인스톨 해야한다,\n",
    "\n",
    "#pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/64/3bba86ca49ef21a4add11a4d37e3f6cd05d2e61d207ebe26a8a96b340826/tensorflow_hub-0.6.0-py2.py3-none-any.whl (84kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in d:\\user\\envs\\tensorflow\\lib\\site-packages (from tensorflow-hub) (1.16.4)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\user\\envs\\tensorflow\\lib\\site-packages (from tensorflow-hub) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in d:\\user\\envs\\tensorflow\\lib\\site-packages (from tensorflow-hub) (3.9.0)\n",
      "Requirement already satisfied: setuptools in d:\\user\\envs\\tensorflow\\lib\\site-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "#세션을 초기화,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
    "# 텐서플로우 허브로부터 ELMo를 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 필요한 것들을 임포트 하였습니다 이제 데이터를 불러오고, 5개만 출력해 보도록 하자\n",
    "\n",
    "파일 다운로드 링크 : https://www.kaggle.com/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('datasets/spam.csv',encoding = 'latin-1')\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 필요한건 v2열과 v1열이다 , v1열은 숫자 레이블로 바꿔야할 필요가 있기 때문에 이를 각각 X_data와 Y_data로 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = list(data['v1'])\n",
    "x_data = list(data['v2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 열을 X_data 에 저장합니다 v1 열에 있는 ham과 spam 레이블을 각각 숫자 0과 1로 바꾸고 y_data에 저장합니다. 정상적으로 저장되었는지 이를 각각 5개만 출력해보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " 'Ok lar... Joking wif u oni...',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'U dun say so early hor... U c already then say...',\n",
       " \"Nah I don't think he goes to usf, he lives around here though\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n",
      "4457\n",
      "1115\n"
     ]
    }
   ],
   "source": [
    "#훈련 데이터와 테스트 데이터를 8:2 비율로 분할한다\n",
    "print(len(x_data))\n",
    "n_of_train = int(len(x_data) * 0.8)\n",
    "n_of_test = int(len(x_data) - n_of_train)\n",
    "\n",
    "print(n_of_train)\n",
    "print(n_of_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4457개의 훈련데이터 1115개의 테스트 데이터의 양으로 하여 데이터를 분할 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.asarray(x_data[:n_of_train]) #X_data 데이터 중에서 앞의 4457개의 데이터만 저장\n",
    "y_train = np.asarray(y_data[:n_of_train]) #y_data 데이터 중에서 앞의 4457개의 데이터만 저장\n",
    "x_test = np.asarray(x_data[n_of_train:]) #X_data 데이터 중에서 뒤의 1115개의 데이터만 저장\n",
    "y_test = np.asarray(y_data[n_of_train:]) #y_data 데이터 중에서 뒤의 1115개의 데이터만 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 ELMo 와 설계한 모델을 연결하는 작업들을 진행해보자, ELMo는 텐서플로우 허브로부터 가져온 것이기 떄문에 케라스에서 사용하기 위해서는 케라스에서 사요할 수 있도록 변환해주는 작업들이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELMoEmbedding(x):\n",
    "    return elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature=\"default\")[\"default\"]\n",
    "# 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 22:06:09.920939 19184 deprecation_wrapper.py:119] From D:\\user\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0913 22:06:09.926126 19184 deprecation_wrapper.py:119] From D:\\user\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0913 22:06:14.221179 19184 deprecation_wrapper.py:119] From D:\\user\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0913 22:06:14.304988 19184 deprecation_wrapper.py:119] From D:\\user\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0913 22:06:14.345767 19184 deprecation_wrapper.py:119] From D:\\user\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0913 22:06:14.354775 19184 deprecation.py:323] From D:\\user\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 모델을 설계 하자\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Lambda, Input\n",
    "\n",
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n",
    "hidden_layer = Dense(256, activation='relu')(embedding_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer)\n",
    "model = Model(inputs=[input_text], outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4457/4457 [==============================] - 728s 163ms/step - loss: 0.0769 - acc: 0.9744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=1, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 156s 140ms/step\n",
      "\n",
      " 테스트 정확도: 0.9731\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE & WordPiece & SentencePiece 임베딩은 다음에 알아보도록 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
