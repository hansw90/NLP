{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation & Seq2Seq, Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Intro\n",
    "- 새로운 문제 소개 : Machine Translation (MT)\n",
    "- 새로운 신경망 구조 소개 : Seq2Seq\n",
    "- 새로운 신경망 기법 소개 : Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기계번역의 역사\n",
    "1. 1950s : Early Machine Translation\n",
    "   -  Early dictionary를 활용한 Rule-based 의 번역 시스템\n",
    "   -  Russian -> English\n",
    "   \n",
    "2. 1990s - 2010s : Statistical Machine Translation\n",
    "    - 서로 다른 두 언어의 Parallel 데이터로부터 확률 모델 구축\n",
    "    \n",
    "3. 2014s : Neural Machine Translation \n",
    "    - Seq2Seq 모델 구조를 사용한 End to End 모델\n",
    "    \n",
    "### //// 용어 정리 ///  \n",
    "__Rule-based  learning (규칙기반 학습)__ :  주어진 입력에 대해서 결과값을 도출하는 방법으로 if-then 방식이라고도 한다. 확고한 규칙(rule)에 따라 학습 및 예측을 하는 방법이다.  \n",
    "\n",
    "__End to End 모델__ :\n",
    "1. end to end 딥러닝은 자료처리 시스템/ 학습 시스템에서 여러 단계의 필요한 처리과정을 한번에 처리한다. 즉, 데이터만 입력하고 원하는 목적을 학습시키는 것,\n",
    "2. 이는 기존의 처리 파이프라인 중 일부를 대체할 수 도 있다, 다만 end to end 딥러닝을 사용하기 위해서는 엄청나게 많은 양의 데이터가 필요하다.\n",
    "3. 하지만 end to end 딥러닝이 만능은 아니다. 단계를 나눠 학습 시킬때 더 효율적인 결과가 나올 수도 있다. 이유는 아래와 같다.\n",
    "    - 복잡한 문제를 분리하여 각각의 간단한 문제로 바꾼다.\n",
    "    - 데이터의 정보가 각각의 작업에 더 적합되게 사용된다.\n",
    "4. 따라서, 순수 end to end 딥러닝 보다는 문제를 쪼개서 해결 하는 것이 좋다.\n",
    "\n",
    "__파이프라인__ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Statistical Machine Translation (SMT)\n",
    "\n",
    "- Core idea : 주어진 parallel 데이터로 부터 확률 모델 (Probabilistic Model) 을 구축\n",
    "- Suppose : French -> English\n",
    "\n",
    "![](img/20200325_085654.png)\n",
    "\n",
    "- Question1 : 어떻게 Tranlation Model P[F|E] 를 학습할 수 있을까?\n",
    "- Answer  : 대량의 Parallel data 가 필요하다.    \n",
    "\n",
    "\n",
    "- Question2 : 충분한 paraller 데이터가 있다고 할때, 어떻게 Tranlation Model P[F|E] 를 학습할 수 있을까?  \n",
    "- Answer : Alignment 를 학습한다.\n",
    "\n",
    "__Alignment__ : Source ~ Target 언어 문장 Pair 에서 일치하는 단어 또는 구를 찾아서 짝을 지어주는 작업이다.\n",
    "![](img/20200325_092023.png)\n",
    "\n",
    "__그러나 Alignment 학습은 어렵다__\n",
    "- Alignment 매칭은 many to one, one to many 등 다양하게 pair 되기도 하기도 한다. \n",
    "![](img/20200325_093111.png)\n",
    "\n",
    "__Decoding for SMT__   \n",
    "decoding 과정이란 아래 그림에서 argmaxE 를 찾는 과정을 말한다.  \n",
    "- Question : 어떤 문장이 Best E인지 찾을 수 있을까?\n",
    "- Answer : 모든 가능한 E에 대하여 확률을 계산하는 것은 매우 계산 비용이 많이 든다. 따라서 Beam-Search 와 같은 Heuristic 한 Search Algorithm 을 사용하여 계산량을 줄이는 방향으로 Decoding 한다.\n",
    "\n",
    "![](img/20200325_093557.png)\n",
    "\n",
    "__SMT의 단점!!__  \n",
    "1. 구조가 너무 복잡하다. (The best systems were extremely complex)  \n",
    "2. 여러개의 서브 컴포넌트들로 디자인 되어 있다. (System had separately-designed subcomponents)\n",
    "3. 피쳐 앤지니어링이 많이 필요하다.\n",
    "4. 3번의 이유로 인해 사람의 노가다가 필요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위의 문제들을 해결하기 위해선??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Neural Machine Translation\n",
    "what is NMT?\n",
    "\n",
    "- 여러개의 subcomponents 로 분리하여 구성하지 않고, 하나의 Neural Net 모델로 MT 수행,\n",
    "- 2014년 이후 ,본격적으로 연구되어 현재는 대부분 NMT 기반의 번역기가 사용되고 있다.\n",
    "- Encoder - Decoder 2개의 RNN 을 이어 부인 Seq2Seq 모델이 기본 형태 \n",
    "- 기존 SMT 방식에 비해, 더 높은 성능을 보인다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __seq2seq__\n",
    "\n",
    "- Seq2Seq Model architecture\n",
    "    - Encoder - Decoder 2개의 RNN 을 연결한 형태.\n",
    "\n",
    "![](img/20200325_095506.png)    \n",
    "\n",
    "- Encoder \n",
    "    - Source sentence 의 context 정보를 요약\n",
    "    - Context 요약 정보인 Last hidden state 를 Decoder 로 전달하여 initial hidden state 로 사용 한다.\n",
    "    \n",
    "- Decoder \n",
    "    - Source sentence 의 context 요약 정보를 기반으로, target sentence를 생성\n",
    "    - Decoder RNN 은 Conditional Language Model 의 한종류 \n",
    "    - p(y|x)를 direct로 계산 가능하다.\n",
    "    \n",
    "- How to train seq2seq model?\n",
    "    - seq2seq 모델은 하나의 시스템으로 최적화 된다.\n",
    "    - End to End 로 Backpropagation이 이루어 진다.\n",
    "    \n",
    "![](img/20200325_100350.png)\n",
    "\n",
    "- Teacher -Forcing\n",
    "    - 모델 학습시 Decoder 에서 실제 정답 데이터를 사용할 비율을 설정하는 하이퍼 파라미터,\n",
    "    - 초기에 잘못 생성한 단어로 인해 계속해서 잘못된 ㅁ누장이 생성되어 학습이 잘 안되는 것을 방지하기 위함\n",
    "    - 높게 설정할 수록 빠른 학습은 가능하지만 학습데이터에 Overfiting 이 되는 문제 또한 발생하게 되므로 적절히 설정한다.\n",
    "    - Inference 단계에서는 Teacher-Forcing ratio = 0 으로 설정하여, 모델이 생성한 단어만으로 다음 단어 추론한다.  \n",
    "    \n",
    "    traning 단계 : 입력 데이터를 통해 모델을 학습하는 과정  \n",
    "    inference 단계 : 학습된 모델로 인식 등의 서비스를 수행하는 과정\n",
    "    \n",
    "    \n",
    "__Searching Algorithm for Decoding__\n",
    "- Exhaustive Search Decoding \n",
    "    - 모든 가능한 조합의 문장을 전부 생성하고 확률이 가장 높은 문장을 선택\n",
    "    - 모든 경우를 다 계산하는 것은 비효율적이다, 따라서 효율적인 Searching algorithm 을 선택할 필요가 있다.\n",
    "    \n",
    "- Greedy Search Decoding \n",
    "    - 매 스텝마다 가장 확률이 높은 다음 단어를 선택\n",
    "    - Greedy Algorithm 이 항상 최적해를 보장 하지는 않는다.\n",
    "    ![](img/20200325_104938.png)\n",
    "    현재 스텝에서 모든 단어들에 대해서 score 값을 계산후 score 값이 가장 높은 단어 선택 후 나머지 단어들을 제거 하는 방식이다.  그리고 마지막 <END> 토큰이 나올 때 까지 위 과정을 반복 수행한다. \n",
    "    \n",
    "    \n",
    "- Beam Search Decoding\n",
    "    - 매 스텝마다 가장 확률이 높은 k개의 다음 단어를 선택 후 ,  다음 스탭 단어들의 확률을 예측\n",
    "    - k*V 개의 후보군 중 또다시 확률이 높은 k개의 후보군만 선택하고 나머지 단어는 제거\n",
    "    - 최종적으로 k*k 개의 후보군 중에서 가장 확률이 높은 k개의 후보군 만을 유지 \n",
    "    - Beam Search 또한 항상 최적해를 보장하진 않지만, Exhaustive search 보다는 효율적\n",
    "    - Greedy serach 가 놓칠 수 있는 더 나은 후보군을 유지할 수 있음,\n",
    "    ![](img/20200325_105859.png)\n",
    "    \n",
    "현재 스텝에서 모든 단어들에 대한 score 값을 계산 후 , score 값이 좋은 k개의 단어 선택 후 나머지 단어를 제거 하는 방식, 다음 스텝의 모든 단어들에 대해 score 값을 계산하고 확률이 높은 k개 선택, 매 스텝 k개의 후보군을 유지하며 <END> 토큰이 나올때 까지 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
