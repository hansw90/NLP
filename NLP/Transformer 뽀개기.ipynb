{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Lambda, Layer, Embedding, LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ashgh\\\\해오라기'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working directory 를 통일하기\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\ashgh\\\\해오라기')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ashgh\\\\해오라기'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "NUM_WORDS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot-Scaled Attention\n",
    "\n",
    "![scaled dot product](https://cdn-images-1.medium.com/freeze/max/1000/1*ke6i8PgYamVVKedNbcmjVA.png?q=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotScaledAttention(Layer) :\n",
    "    def __init__(self, d_emb, d_reduced, masked=False) :\n",
    "        super().__init__()\n",
    "        self.q = Dense(d_reduced, input_shape=(-1, d_emb))\n",
    "        self.k = Dense(d_reduced, input_shape=(-1, d_emb))\n",
    "        self.v = Dense(d_reduced, input_shape=(-1, d_emb))\n",
    "        self.masked = masked\n",
    "        \n",
    "        self.scale = Lambda(lambda x : x/np.sqrt(d_reduced))\n",
    "        \n",
    "        \n",
    "    # (q, k ,v)\n",
    "    # self attention 을 할때 미래의 것을 참조하면 안되기 때문에 \n",
    "    # 그부분을 고려한 코드\n",
    "    # git 에서 참조\n",
    "    def call(self, x, training =None, mask = None) :\n",
    "        q = self.scale(self.q(x[0]))\n",
    "        k = self.k(x[1])\n",
    "        v = self.v(x[2])\n",
    "        \n",
    "        # k를 transpose 한 후 q와 matrix multiplication 해준다\n",
    "        k_T = tf.transpose(k, perm=[0,2,1])\n",
    "        # 결과는 두개를 inner product 한것\n",
    "        comp = tf.matmul(q, k_T)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.masked :\n",
    "            length = tf.shape(comp)[-1]\n",
    "            # 미래에 해당하면 마이너스 인피니티를 넣어 softmax 시에 0이 된다.\n",
    "            mask = tf.fill((length, length), -np.inf)\n",
    "            mask = tf.linalg.band_part(mask, 0, -1) # Get upper triangle\n",
    "            mask = tf.linalg.set_diag(mask, tf.zeros((length))) # Set diagnoal to zeros\n",
    "            comp += mask\n",
    "            \n",
    "        comp = tf.nn.softmax(comp, axis)\n",
    "        \n",
    "        \n",
    "        return tf.matmul(comp, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer) :\n",
    "    def __init__(self, num_head, d_emb, d_reduced, masked=False) :\n",
    "        super().__init__()\n",
    "        # attention 을 여러개 만들자\n",
    "        self.attention_list = list()\n",
    "        for _ in range(num_head) :\n",
    "            self.attention_list.append(DotScaledAttention(d_emb, d_reduced, masked))\n",
    "       \n",
    "        # 위결과를 concat 후 다시 projection 하는 과정\n",
    "        self.linear = Dense(d_emb, input_shape=(-1, num_head * d_reduced))\n",
    "        \n",
    "    def call(self, x, traning=None, mask=None) :\n",
    "        \n",
    "        attention_list = [a(x) for a in self.attention_list]\n",
    "        concat = tf.concat(attention_list, axis=-1)\n",
    "        return self.linear(concat)\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "![encoder](https://miro.medium.com/max/329/1*4HJt3iD5tbtf9wZFuDrM-Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer) :\n",
    "    def __init__(self, num_head, d_reduced) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.d_r = d_reduced\n",
    "\n",
    "    # build는 레이어를 초기화하며\n",
    "    # input_shape 가 처음 들어왔을때 어떻게 빌드 할것인가\n",
    "    def build(self, input_shape) :\n",
    "        # 왜 input_shape[-1] 인가?\n",
    "        # x의 dimention을 구해야 하기 때문 ,\n",
    "        # MHA 의 d_emb 파라미터 값이다,\n",
    "        self.multi_attention = MultiHeadAttention(self.num_head, input_shape[-1], self.d_r)\n",
    "        self.layer_norm1 = LayerNomalization(input_shape= input_shape)\n",
    "        self.dense1 = Dense(input_shape[-1] * 4, input_shape = input_shape, activation ='relu')\n",
    "        self.dense2 = Dense(input_shape[-1] \n",
    "                            , input_shape = self.dense1.compute_output_shape(input_shape))\n",
    "        self.layer_norm1 = LayerNomalization(input_shape= input_shape)\n",
    "        \n",
    "        \n",
    "        super().build(input_shape)\n",
    "    # call 은 입력을 처리해서 결과를 반환하는 메소드\n",
    "    def call(self, x, training=None, mask=None) :\n",
    "        # 인코더에선 selfattention 구조이므로 xxx\n",
    "        # 아래 두줄은 residual 구조\n",
    "        h = self.multi_attention(x, x, x)\n",
    "        ln1 = self.layer_norm1(x+h)\n",
    "        \n",
    "        h = self.linear2(self.linear1(ln1))\n",
    "        ln2 = self.layer_norm1(h+ln1)\n",
    "        return ln2\n",
    "        \n",
    "    def compute_output_shape(self, input_shape) :\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
