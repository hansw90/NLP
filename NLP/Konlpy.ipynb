{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.morphs('단독입찰보다 복수입찰의 경우'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['항공기', '체계', '종합', '개발']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.nouns(u'유일하게 항공기 체계 종합 개발'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.phrases(u'날카로운 분석과 신뢰감 있는 진행으로'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "print(twitter.pos(u'이것도 되나욬 ㅋㅋㅋ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "print(twitter.pos(u'이것도 되나욬ㅋㅋ', norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "[('shewasaboy', 'Alpha')]\n"
     ]
    }
   ],
   "source": [
    "print(twitter.pos(u'이것도 되나욬ㅋㅋ', norm=True, stem=True))\n",
    "print(twitter.pos(u'shewasaboy', norm=True, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Okt' object has no attribute 'analyze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0d2adfe35f20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'아이런것도 다 해주나요?????'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Okt' object has no attribute 'analyze'"
     ]
    }
   ],
   "source": [
    "print(twitter.analyze(u'아이런것도 다 해주나요?????'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constitution.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "kolaw.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "\n",
      "유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로\n"
     ]
    }
   ],
   "source": [
    "c = kolaw.open('constitution.txt').read()\n",
    "print(c[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1809890.txt',\n",
       " '1809891.txt',\n",
       " '1809892.txt',\n",
       " '1809893.txt',\n",
       " '1809894.txt',\n",
       " '1809895.txt',\n",
       " '1809896.txt',\n",
       " '1809897.txt',\n",
       " '1809898.txt',\n",
       " '1809899.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.corpus import kobill\n",
    "kobill.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지방공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "\n",
      " 의 안\n",
      " 번 호\n"
     ]
    }
   ],
   "source": [
    "d = kobill.open('1809890.txt').read()\n",
    "print(d[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "# mecab = Mecab()\n",
    "okt = Okt()\n",
    "\n",
    "# 이 캘래스들은 다음과 같은 메서드를 공통적으로 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국헌법', '유구', '역사', '전통', '빛', '우리', '대한국민', '3·1운동']\n",
      "['대한', '대한민국', '대한민국헌법', '민국', '헌법', '유구', '역사', '전통', '우리', '국민', '3', '1', '1운동', '운동']\n",
      "['대한민국', '헌법', '유구', '역사', '전통', '우리', '국민', '운동']\n"
     ]
    }
   ],
   "source": [
    "# 명사 추출\n",
    "# 문자열에서 명사만 출출하려면 noun 명령어를 사용한다.\n",
    "\n",
    "\n",
    "print(hannanum.nouns(c[:40]))\n",
    "print(kkma.nouns(c[:40]))\n",
    "print(okt.nouns(c[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국헌법', '유구', '하', 'ㄴ', '역사', '와', '전통', '에', '빛', '나는', '우리', '대한국민', '은', '3·1운동', '으로']\n",
      "['대한민국', '헌법', '유구', '하', 'ㄴ', '역사', '와', '전통', '에', '빛나', '는', '우리', '대하', 'ㄴ', '국민', '은', '3', '·', '1', '운동', '으로']\n",
      "['대한민국', '헌법', '\\n\\n', '유구', '한', '역사', '와', '전통', '에', '빛나는', '우리', '대', '한', '국민', '은', '3', '·', '1', '운동', '으로']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 추출\n",
    "# 명사 뿐 아니라 모든 품사의 형태소를 알아내려면 morphs 라는 명령어를 사용한다.\n",
    "\n",
    "print(hannanum.morphs(c[:40]))\n",
    "print(kkma.morphs(c[:40]))\n",
    "print(okt.morphs(c[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n품사 부착\\npos 명령을 사용하면 품사 부착을 한다.\\n한국어 품사 태그세트로는 \"21세기 세종계획 품사 태그세트\" 를 비롯하여 다양한 품사 태그세트가 있다. 형태소 분석기마다 사용하는 품사 태그가 다르므로 각 형태소 분석기ㅔ 대한 문서를 참조한다.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "품사 부착\n",
    "pos 명령을 사용하면 품사 부착을 한다.\n",
    "한국어 품사 태그세트로는 \"21세기 세종계획 품사 태그세트\" 를 비롯하여 다양한 품사 태그세트가 있다. \n",
    "형태소 분석기마다 사용하는 품사 태그가 다르므로 각 형태소 분석기ㅔ 대한 문서를 참조한다.\n",
    "\"\"\"\n",
    "\n",
    "hann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대한민국헌법', 'N'),\n",
       " ('유구', 'N'),\n",
       " ('하', 'X'),\n",
       " ('ㄴ', 'E'),\n",
       " ('역사', 'N'),\n",
       " ('와', 'J'),\n",
       " ('전통', 'N'),\n",
       " ('에', 'J'),\n",
       " ('빛', 'N'),\n",
       " ('나는', 'J'),\n",
       " ('우리', 'N'),\n",
       " ('대한국민', 'N'),\n",
       " ('은', 'J'),\n",
       " ('3·1운동', 'N'),\n",
       " ('으로', 'J')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.pos(c[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대한민국', 'NNG'),\n",
       " ('헌법', 'NNG'),\n",
       " ('유구', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('역사', 'NNG'),\n",
       " ('와', 'JC'),\n",
       " ('전통', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('빛나', 'VV'),\n",
       " ('는', 'ETD'),\n",
       " ('우리', 'NNM'),\n",
       " ('대하', 'VV'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('국민', 'NNG'),\n",
       " ('은', 'JX'),\n",
       " ('3', 'NR'),\n",
       " ('·', 'SP'),\n",
       " ('1', 'NR'),\n",
       " ('운동', 'NNG'),\n",
       " ('으로', 'JKM')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.pos(c[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대한민국', 'NNP'),\n",
       " ('헌법', 'NNP'),\n",
       " ('유구', 'XR'),\n",
       " ('하', 'XSA'),\n",
       " ('ㄴ', 'ETM'),\n",
       " ('역사', 'NNG'),\n",
       " ('와', 'JC'),\n",
       " ('전통', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('빛나', 'VV'),\n",
       " ('는', 'ETM'),\n",
       " ('우리', 'NP'),\n",
       " ('대하', 'VV'),\n",
       " ('ㄴ', 'ETM'),\n",
       " ('국민', 'NNP'),\n",
       " ('은', 'JX'),\n",
       " ('3', 'SN'),\n",
       " ('·', 'SP'),\n",
       " ('1', 'SN'),\n",
       " ('운동', 'NNP'),\n",
       " ('으로', 'JKB')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# komoran은 빈줄이 있으면 에러가 남\n",
    "komoran.pos(\"\\n\".join([s for s in c[:40].split(\"\\n\") if s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대한민국', 'Noun'),\n",
       " ('헌법', 'Noun'),\n",
       " ('\\n\\n', 'Foreign'),\n",
       " ('유구', 'Noun'),\n",
       " ('한', 'Josa'),\n",
       " ('역사', 'Noun'),\n",
       " ('와', 'Josa'),\n",
       " ('전통', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('빛나는', 'Verb'),\n",
       " ('우리', 'Noun'),\n",
       " ('대', 'Modifier'),\n",
       " ('한', 'Modifier'),\n",
       " ('국민', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('3', 'Number'),\n",
       " ('·', 'Punctuation'),\n",
       " ('1', 'Number'),\n",
       " ('운동', 'Noun'),\n",
       " ('으로', 'Josa')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos(c[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hannanum-기호</th>\n",
       "      <th>Hannanum-품사</th>\n",
       "      <th>Kkma-기호</th>\n",
       "      <th>Kkma-품사</th>\n",
       "      <th>Komoran-기호</th>\n",
       "      <th>Komoran-품사</th>\n",
       "      <th>OKT-기호</th>\n",
       "      <th>OKT-품사</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>어미</td>\n",
       "      <td>EC</td>\n",
       "      <td>연결 어미</td>\n",
       "      <td>EC</td>\n",
       "      <td>연결 어미</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>형용사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EC</td>\n",
       "      <td>연결 어미</td>\n",
       "      <td>ECD</td>\n",
       "      <td>의존적 연결 어미</td>\n",
       "      <td>EF</td>\n",
       "      <td>종결 어미</td>\n",
       "      <td>Adverb</td>\n",
       "      <td>부사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EF</td>\n",
       "      <td>종결 어미</td>\n",
       "      <td>ECE</td>\n",
       "      <td>대등 연결 어미</td>\n",
       "      <td>EP</td>\n",
       "      <td>선어말어미</td>\n",
       "      <td>Alpha</td>\n",
       "      <td>알파벳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EP</td>\n",
       "      <td>선어말어미</td>\n",
       "      <td>ECS</td>\n",
       "      <td>보조적 연결 어미</td>\n",
       "      <td>ETM</td>\n",
       "      <td>관형형 전성 어미</td>\n",
       "      <td>Conjunction</td>\n",
       "      <td>접속사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ET</td>\n",
       "      <td>전성 어미</td>\n",
       "      <td>EF</td>\n",
       "      <td>종결 어미</td>\n",
       "      <td>ETN</td>\n",
       "      <td>명사형 전성 어미</td>\n",
       "      <td>Determiner</td>\n",
       "      <td>관형사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>외국어</td>\n",
       "      <td>EFA</td>\n",
       "      <td>청유형 종결 어미</td>\n",
       "      <td>IC</td>\n",
       "      <td>감탄사</td>\n",
       "      <td>Eomi</td>\n",
       "      <td>어미</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "      <td>독립언</td>\n",
       "      <td>EFI</td>\n",
       "      <td>감탄형 종결 어미</td>\n",
       "      <td>JC</td>\n",
       "      <td>접속 조사</td>\n",
       "      <td>Exclamation</td>\n",
       "      <td>감탄사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>II</td>\n",
       "      <td>감탄사</td>\n",
       "      <td>EFN</td>\n",
       "      <td>평서형 종결 어미</td>\n",
       "      <td>JKB</td>\n",
       "      <td>부사격 조사</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>외국어, 한자 및 기타기호</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>J</td>\n",
       "      <td>관계언</td>\n",
       "      <td>EFO</td>\n",
       "      <td>명령형 종결 어미</td>\n",
       "      <td>JKC</td>\n",
       "      <td>보격 조사</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>트위터 해쉬태그</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JC</td>\n",
       "      <td>격조사</td>\n",
       "      <td>EFQ</td>\n",
       "      <td>의문형 종결 어미</td>\n",
       "      <td>JKG</td>\n",
       "      <td>관형격 조사</td>\n",
       "      <td>Josa</td>\n",
       "      <td>조사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JP</td>\n",
       "      <td>서술격 조사</td>\n",
       "      <td>EFR</td>\n",
       "      <td>존칭형 종결 어미</td>\n",
       "      <td>JKO</td>\n",
       "      <td>목적격 조사</td>\n",
       "      <td>KoreanParticle</td>\n",
       "      <td>(ex: ㅋㅋ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JX</td>\n",
       "      <td>보조사</td>\n",
       "      <td>EP</td>\n",
       "      <td>선어말 어미</td>\n",
       "      <td>JKQ</td>\n",
       "      <td>인용격 조사</td>\n",
       "      <td>Noun</td>\n",
       "      <td>명사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M</td>\n",
       "      <td>수식언</td>\n",
       "      <td>EPH</td>\n",
       "      <td>존칭 선어말 어미</td>\n",
       "      <td>JKS</td>\n",
       "      <td>주격 조사</td>\n",
       "      <td>Number</td>\n",
       "      <td>숫자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MA</td>\n",
       "      <td>부사</td>\n",
       "      <td>EPP</td>\n",
       "      <td>공손 선어말 어미</td>\n",
       "      <td>JKV</td>\n",
       "      <td>호격 조사</td>\n",
       "      <td>PreEomi</td>\n",
       "      <td>선어말어미</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MM</td>\n",
       "      <td>관형사</td>\n",
       "      <td>EPT</td>\n",
       "      <td>시제 선어말 어미</td>\n",
       "      <td>JX</td>\n",
       "      <td>보조사</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>구두점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>N</td>\n",
       "      <td>체언</td>\n",
       "      <td>ET</td>\n",
       "      <td>전성 어미</td>\n",
       "      <td>MAG</td>\n",
       "      <td>일반 부사</td>\n",
       "      <td>ScreenName</td>\n",
       "      <td>트위터 아이디</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NB</td>\n",
       "      <td>의존명사</td>\n",
       "      <td>ETD</td>\n",
       "      <td>관형형 전성 어미</td>\n",
       "      <td>MAJ</td>\n",
       "      <td>접속 부사</td>\n",
       "      <td>Suffix</td>\n",
       "      <td>접미사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NC</td>\n",
       "      <td>보통명사</td>\n",
       "      <td>ETN</td>\n",
       "      <td>명사형 전성 어미</td>\n",
       "      <td>MM</td>\n",
       "      <td>관형사</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>미등록어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NN</td>\n",
       "      <td>수사</td>\n",
       "      <td>IC</td>\n",
       "      <td>감탄사</td>\n",
       "      <td>NA</td>\n",
       "      <td>분석불능범주</td>\n",
       "      <td>Verb</td>\n",
       "      <td>동사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NP</td>\n",
       "      <td>대명사</td>\n",
       "      <td>JC</td>\n",
       "      <td>접속 조사</td>\n",
       "      <td>NF</td>\n",
       "      <td>명사추정범주</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NQ</td>\n",
       "      <td>고유명사</td>\n",
       "      <td>JK</td>\n",
       "      <td>조사</td>\n",
       "      <td>NNB</td>\n",
       "      <td>의존 명사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>P</td>\n",
       "      <td>용언</td>\n",
       "      <td>JKC</td>\n",
       "      <td>보격 조사</td>\n",
       "      <td>NNG</td>\n",
       "      <td>일반 명사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PA</td>\n",
       "      <td>형용사</td>\n",
       "      <td>JKG</td>\n",
       "      <td>관형격 조사</td>\n",
       "      <td>NNP</td>\n",
       "      <td>고유 명사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PV</td>\n",
       "      <td>동사</td>\n",
       "      <td>JKI</td>\n",
       "      <td>호격 조사</td>\n",
       "      <td>NP</td>\n",
       "      <td>대명사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PX</td>\n",
       "      <td>보조 용언</td>\n",
       "      <td>JKM</td>\n",
       "      <td>부사격 조사</td>\n",
       "      <td>NR</td>\n",
       "      <td>수사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>S</td>\n",
       "      <td>기호</td>\n",
       "      <td>JKO</td>\n",
       "      <td>목적격 조사</td>\n",
       "      <td>NV</td>\n",
       "      <td>용언추정범주</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>X</td>\n",
       "      <td>접사</td>\n",
       "      <td>JKQ</td>\n",
       "      <td>인용격 조사</td>\n",
       "      <td>SE</td>\n",
       "      <td>줄임표</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>XP</td>\n",
       "      <td>접두사</td>\n",
       "      <td>JKS</td>\n",
       "      <td>주격 조사</td>\n",
       "      <td>SF</td>\n",
       "      <td>마침표, 물음표, 느낌표</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>XS</td>\n",
       "      <td>접미사</td>\n",
       "      <td>JX</td>\n",
       "      <td>보조사</td>\n",
       "      <td>SH</td>\n",
       "      <td>한자</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>MA</td>\n",
       "      <td>부사</td>\n",
       "      <td>SL</td>\n",
       "      <td>외국어</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>NNG</td>\n",
       "      <td>보통명사</td>\n",
       "      <td>VCP</td>\n",
       "      <td>긍정 지정사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>NNM</td>\n",
       "      <td>단위 의존 명사</td>\n",
       "      <td>VV</td>\n",
       "      <td>동사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>NNP</td>\n",
       "      <td>고유명사</td>\n",
       "      <td>VX</td>\n",
       "      <td>보조 용언</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>NP</td>\n",
       "      <td>대명사</td>\n",
       "      <td>XPN</td>\n",
       "      <td>체언 접두사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>NR</td>\n",
       "      <td>수사</td>\n",
       "      <td>XR</td>\n",
       "      <td>어근</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>OH</td>\n",
       "      <td>한자</td>\n",
       "      <td>XSA</td>\n",
       "      <td>형용사 파생 접미사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>OL</td>\n",
       "      <td>외국어</td>\n",
       "      <td>XSN</td>\n",
       "      <td>명사파생 접미사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>ON</td>\n",
       "      <td>숫자</td>\n",
       "      <td>XSV</td>\n",
       "      <td>동사 파생 접미사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>SE</td>\n",
       "      <td>줄임표</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>SF</td>\n",
       "      <td>마침표, 물음표, 느낌표</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>SO</td>\n",
       "      <td>붙임표(물결,숨김,빠짐)</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>SP</td>\n",
       "      <td>쉼표,가운뎃점,콜론,빗금</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>SS</td>\n",
       "      <td>따옴표,괄호표,줄표</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>SW</td>\n",
       "      <td>기타기호 (논리수학기호,화폐기호)</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>UN</td>\n",
       "      <td>명사추정범주</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>VA</td>\n",
       "      <td>형용사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>VC</td>\n",
       "      <td>지정사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>VCN</td>\n",
       "      <td>부정 지정사, 형용사 '아니다'</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>VCP</td>\n",
       "      <td>긍정 지정사, 서술격 조사 '이다'</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>VV</td>\n",
       "      <td>동사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>VX</td>\n",
       "      <td>보조 용언</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>VXA</td>\n",
       "      <td>보조 형용사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>VXV</td>\n",
       "      <td>보조 동사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>XP</td>\n",
       "      <td>접두사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>XPN</td>\n",
       "      <td>체언 접두사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>XPV</td>\n",
       "      <td>용언 접두사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>XR</td>\n",
       "      <td>어근</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>XSA</td>\n",
       "      <td>형용사 파생 접미사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>XSN</td>\n",
       "      <td>명사파생 접미사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>XSV</td>\n",
       "      <td>동사 파생 접미사</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hannanum-기호 Hannanum-품사 Kkma-기호     Kkma-품사 Komoran-기호 Komoran-품사  \\\n",
       "0            E          어미      EC       연결 어미         EC      연결 어미   \n",
       "1           EC       연결 어미     ECD   의존적 연결 어미         EF      종결 어미   \n",
       "2           EF       종결 어미     ECE    대등 연결 어미         EP      선어말어미   \n",
       "3           EP       선어말어미     ECS   보조적 연결 어미        ETM  관형형 전성 어미   \n",
       "4           ET       전성 어미      EF       종결 어미        ETN  명사형 전성 어미   \n",
       "..         ...         ...     ...         ...        ...        ...   \n",
       "62           *           *     XPV      용언 접두사          *          *   \n",
       "63           *           *      XR          어근          *          *   \n",
       "64           *           *     XSA  형용사 파생 접미사          *          *   \n",
       "65           *           *     XSN    명사파생 접미사          *          *   \n",
       "66           *           *     XSV   동사 파생 접미사          *          *   \n",
       "\n",
       "         OKT-기호 OKT-품사  \n",
       "0     Adjective    형용사  \n",
       "1        Adverb     부사  \n",
       "2         Alpha    알파벳  \n",
       "3   Conjunction    접속사  \n",
       "4    Determiner    관형사  \n",
       "..          ...    ...  \n",
       "62            *      *  \n",
       "63            *      *  \n",
       "64            *      *  \n",
       "65            *      *  \n",
       "66            *      *  \n",
       "\n",
       "[67 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagsets = pd.DataFrame()\n",
    "N = 67\n",
    "tagsets[\"Hannanum-기호\"] = list(hannanum.tagset.keys()) + list(\"*\" * (N - len(hannanum.tagset)))\n",
    "tagsets[\"Hannanum-품사\"] = list(hannanum.tagset.values()) + list(\"*\" * (N - len(hannanum.tagset)))\n",
    "tagsets[\"Kkma-기호\"] = list(kkma.tagset.keys()) + list(\"*\" * (N - len(kkma.tagset)))\n",
    "tagsets[\"Kkma-품사\"] = list(kkma.tagset.values()) + list(\"*\" * (N - len(kkma.tagset)))\n",
    "tagsets[\"Komoran-기호\"] = list(komoran.tagset.keys()) + list(\"*\" * (N - len(komoran.tagset)))\n",
    "tagsets[\"Komoran-품사\"] = list(komoran.tagset.values()) + list(\"*\" * (N - len(komoran.tagset)))\n",
    "tagsets[\"OKT-기호\"] = list(okt.tagset.keys()) + list(\"*\" * (N - len(okt.tagset)))\n",
    "tagsets[\"OKT-품사\"] = list(okt.tagset.values()) + list(\"*\" * (N - len(okt.tagset)))\n",
    "tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pos() missing 1 required positional argument: 'phrase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-abf1fa1107b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mKkma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'she was a boy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: pos() missing 1 required positional argument: 'phrase'"
     ]
    }
   ],
   "source": [
    "Kkma.pos('she was a boy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 토큰화(Tokenization) : 문자열에서 단어로 분리시키는 단계\n",
    "2. 불용어 제거 (Stop word elimination) : 전치사, 관사 등 너무 많이 등장하는 단어 등 문장이나 문서의 특징을 표현하는데 불필요한 단어를 삭제하는 단계\n",
    "3. 어간 추출 (Stemming) : 단어의 기본 형태를 추출하는 단계\n",
    "4. 문서 표현 (Representation) : 주어진 문서나 문장을 하나의 벡터로 표현하는 단계, 단어들을 모두 인덱싱(indexing)하고 주어진 문서에 존재하는 단어의 빈도수를 사용하여 문서를 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'왕자', '공주', '좋아한다', '싫어한다', '시녀', '독살한다', '를', '가'}\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    ['왕자', '가', '공주', '를', '좋아한다'],\n",
    "    ['공주', '가', '왕자', '를', '좋아한다'],\n",
    "    ['시녀', '가', '왕자', '를', '싫어한다'],\n",
    "    ['공주', '가', '시녀', '를', '싫어한다'],\n",
    "    ['시녀', '가', '왕자', '를', '독살한다'],\n",
    "    ['시녀', '가', '공주', '를', '독살한다']\n",
    "]\n",
    "words = set(sum(documents, []))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "['가방', '가방에', '가신다', '갔다', '들어', '아버지', '엄마']\n",
      "[[1 0 1 0 1 1 0]\n",
      " [0 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 하나의 Counter Vector 를 만든다.\n",
    "# 이 때 전달하는 인자 min_df는 해당 값보다 빈도수가 낮은 단어는 무시하겠다는 뜻\n",
    "# max_df 인자는 해당 값보다 많이 나오는, 빈도수가 높은 단어는 무시하겠다라는 뜻\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "content = [\"아버지 가 가방 에 들어 가신다\", \"엄마 가 가방에 들어 갔다\"]\n",
    "\n",
    "# fit_transform 함수를 통해 Counter Vector로 만들 수 있다\n",
    "X = vectorizer.fit_transform(content)\n",
    "print(X)\n",
    "# 출력\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 게시물 분류하기\n",
    "\n",
    "#### 01) 카운트 벡터 만들기\n",
    "\n",
    "짧은 글 5개를 이용하여 Count Vector의 거리르 계산해 보도록 하자\n",
    "짧은 글은 아래와 같다.\n",
    "\n",
    " 1. This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
    " 2. Imaging databases provide storage capabilities.\n",
    " 3. Most imaging databases save images permanently.\n",
    " 4. Imaging databases store data.\n",
    " 5. Imaging databases store data. Imaging databases store data. Imaging databases store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 파일 리스트를 불러와 파일을 읽어 list에 저장해준다.\n",
    "# os.listdir 을 통해 해당 디렉토리 하위의 파일 경로를 모두 불러온 후\n",
    "# os.path.join을 통해 디렉토리 경로를 합친 후 read() 로 읽어 list에 담습니다.\n",
    "\n",
    "posts = [\n",
    "    \"This is a toy post about machine learning. Actually, it contains not much interesting stuff.\",\n",
    "    \"Imaging databases provide storage capabilities.\",\n",
    "    \"Most imaging databases save images permanently.\",\n",
    "    \"Imaging databases store data.\",\n",
    "    \"Imaging databases store data. Imaging databases store data. Imaging databases store data.\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-64c8facb5d71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnew_post\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"imaging databases\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_post_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_post\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# transform Vs fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# transform  : 인자로 하나의 문자열을 받는다..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "# transform Vs fit_transform \n",
    "# transform  : 인자로 하나의 문자열을 받는다..\n",
    "# fit_transform  : 인자로 list 형태의 문자열을 받는다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    # norm(): 벡터 간에 유클리드 거리를 계산\n",
    "    return np.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-800c7c70d1f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mpost_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 벡터 사이의 거리를 구하기 위해서는 1차원 배열을 이용해야 하기 때문에 [0], 인덱스를 지정합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_post_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_dist = sys.maxsize\n",
    "best_doc = None\n",
    "best_i = None\n",
    "X_train = np.array([0])\n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    post_vec = X_train.toarray()[i]\n",
    "    # 벡터 사이의 거리를 구하기 위해서는 1차원 배열을 이용해야 하기 때문에 [0], 인덱스를 지정합니다.\n",
    "    d = dist_raw(post_vec, new_post_vec.toarray()[0])\n",
    "    print(\"=== Post %i with dist = %.2f: %s\" %(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 단순하게 상속받는 클래스 이름만 바꿔주면 됩니다.\n",
    "class StemmedCountVector(TfidfVectorizer) :\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVector, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedCountVector(min_df=1, stop_words=\"english\", decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다\n"
     ]
    }
   ],
   "source": [
    "token = re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업.\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n"
     ]
    }
   ],
   "source": [
    "token = okt.morphs(token)\n",
    "# OKT 혀태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣는다\n",
    "\n",
    "word2index= {}\n",
    "bow = []\n",
    "\n",
    "for voca in token :\n",
    "    if voca not in word2index.keys() :\n",
    "        word2index[voca] = len(word2index)\n",
    "        \n",
    "# token 을 읽으면서, word2index 에 없는 (not in ) 단어는 새로 추가하고 , 이미 있는 단어는 ㅓㄴㅁ긴다.\n",
    "        bow.insert(len(word2index)-1,1)\n",
    "# bow 전체에 전부 기본값 1을 넣어준다,  단어의 개수는 최소 1개 이상이기 떄문이다\n",
    "    else :\n",
    "        index=word2index.get(voca)\n",
    "        #재등장하는 단어의 인덱스를 받아온다\n",
    "        bow[index] = bow[index] + 1\n",
    "        #재등장한 단어는 해당 인덱스의 위치에 1을 더한다\n",
    "        \n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서1에 각 단어에 대해서 인덱스를 부여한 결과는 첫번째 출력 결과입니다. 문서1의 BoW는 두번째 출력 결과입니다. 두번째 출력 결과를 보면, 물가상승률의 인덱스는 4이며, 문서1에서 물가상승률은 2번 언급되었기 때문에 인덱스 4(다섯번째 값)에 해당하는 값이 2임을 알 수 있습니다. \n",
    "\n",
    "- (원한다면 한국어에서 불용어에 해당되는 조사들 또한 제거하여 더 정제된 BoW를 만들 수도 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "# 사용자가 직접 정의한 불용어 사용\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "# COuntVetorizer 에서 제공하는 자체 불용어 사용\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "# NLTK 에서 지원하는 불용어 사용\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words =sw)\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
